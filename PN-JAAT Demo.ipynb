{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82976832",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Job Ad Analysis Toolkit \n",
    "\n",
    "## A Demonstration from Unstructured Job Ad Text to Data\n",
    "\n",
    "\n",
    "### Tools: TitleMatch, FirmExtract, TaskMatch, and CREAM\n",
    "\n",
    "Let's code up 990 Job Ads with the following machine tools:\n",
    "\n",
    "1. TitleMatch - Match Job Titles to O-NET Occupation Codes.\n",
    "2. FirmNER - Extracts Candidate Firm Names from Job Ad (or other) Text.\n",
    "3. TaskMatch - Match Job Ad Text to O-NET Task IDs\n",
    "4. CREAM - Develop custom classifiers with embeddings by augmenting a human curated list of 'rules'. Demo uses Education Requirements.\n",
    "\n",
    "### Acknolwedgements\n",
    "\n",
    "This project has received generous support from the National Labor Exchange, the Russell Sage Foundation, the Washington Center for Equitable Growth.\n",
    "\n",
    "Job ad data for this demo is from:\n",
    "\n",
    "```\n",
    "\n",
    "Zhou, Steven, John Aitken, Peter McEachern, and Renee McCauley. “Data from 990 Public Real-World Job Advertisements Organized by O*NET Categories.” Journal of Open Psychology Data 10 (November 21, 2022): 17. https://doi.org/10.5334/jopd.69.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e2dfe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "\n",
    "data = pd.read_excel(\"data/demo/TitleMatch/JobAdsData2022_OSF.xlsx\")\n",
    "data['text'] = data['text'].str.strip()\n",
    "data.text = data.text.replace(r'\\n','. ', regex=True)\n",
    "data.text = data.text.apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "# We will work with the columns 'job_title' and 'text' (a concatenation of all text)\n",
    "data[['job_title','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e63302",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dea394",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Title Match: Match Job Titles to O-NET Occupation Codes\n",
    "\n",
    "From:\n",
    "\n",
    "'Operations Engineer'\n",
    "'ESL Processing Operator'\n",
    "\n",
    "To: \n",
    "\n",
    "[('Operations Engineer', '17-2112.00', 1.0)]\n",
    "[('Processing Operator', '51-3091.00', 0.943)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ffff1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from JAAT import TitleMatch\n",
    "TiM = TitleMatch()\n",
    "\n",
    "data['TitleMatch'] = TiM.get_title(data.job_title.to_list())\n",
    "data['TM_onet_id'] = data.TitleMatch.apply(itemgetter(1)).astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100d649",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data[['job_title', 'TM_onet_id']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fad0bbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Firm Extract: Extracts Candidate Firm Names from Job Ad (or other) Text\n",
    "\n",
    "From:\n",
    "\n",
    "\"First American Equipment Finance, an RBC/City National Company, is a growing, national leader providing equipment leasing and equipment finance services to commercial borrowers in all fifty states. In 2020, the company earned top honors among midsize companies for the Best Companies to Work for in New York for the 3rd consecutive year, and in 2021, added to their accolades by being recognized as the #1 top workplace among midsized companies in Rochester. With national headquarters in Rochester (Woodcliff Office Park, Fairport), First American has approximately 270 employees and manages a $2bn portfolio.  \n",
    "\n",
    "To: \n",
    "\n",
    "{'american', 'first american'}\n",
    "\n",
    "From: \n",
    "\n",
    "As a key member of the Lead Discovery team, reporting to the Head of Lead Discovery, the Scientist, Lead Discovery will develop and implement biochemical assays for internal and external use and collaborate with a cross-functional team of scientists within the company and with external partners to support drug discovery efforts across the Accent portfolio.\t\n",
    "\n",
    "To: \n",
    "{'accent'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51cac47",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from JAAT import FirmExtract\n",
    "FE = FirmExtract()\n",
    "\n",
    "data['FirmExtract'] = FE.get_firm_batch(data.text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa020322",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data[['company','FirmExtract']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b43257",
   "metadata": {},
   "source": [
    "# Task Match: Match Job Ad Text to O-NET Task IDs\n",
    "\n",
    "From text: \n",
    "\n",
    "```\n",
    "Develops software solutions by studying information needs; conferring with users; studying systems flow, data usage, and work processes; investigating problem areas; following the software development lifecycle.\n",
    "Determines operational feasibility by evaluating analysis, problem definition, requirements, solution development, and proposed solutions.\n",
    "Documents and demonstrates solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code.\n",
    "Supports and develops software developers by providing advice, coaching and educational opportunities.\n",
    "Other duties as required.\n",
    "\n",
    "```\n",
    "\n",
    "To O-NET Task IDs:\n",
    "\n",
    "```\n",
    "\n",
    "[('16363', 'Identify operational requirements for new systems to inform selection of technological solutions.'),\n",
    " ('16987', 'Prepare documentation or presentations, including charts, photos, or graphs.'),\n",
    " ('9583', 'Assign duties to other staff and give instructions regarding work methods and routines.')]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ec2c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from JAAT import TaskMatch\n",
    "TM = TaskMatch()\n",
    "\n",
    "data['TaskMatch'] = TM.get_tasks_batch(data.text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99dd08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['TaskMatch']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa200be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CREAM: Adapt a Classification Scheme or Build a Novel Lexicon\n",
    "\n",
    "From initial rules:\n",
    "\n",
    "\n",
    "And natural language in a corpus:\n",
    "\n",
    "\n",
    "\n",
    "To labeled data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae78b09",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "import re\n",
    "import swifter\n",
    "\n",
    "# sbert model\n",
    "#model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "\n",
    "# similarity function\n",
    "# vectorize input text and use cosine similarity to compare to encoded rules\n",
    "def get_sim(model, rule_map, encoded_rules, q):\n",
    "    sim_scores = util.cos_sim(model.encode([q]), encoded_rules)\n",
    "    return dict(zip(rule_map.keys(), sim_scores[0].tolist()))\n",
    "\n",
    "# label via max score\n",
    "def label_from_max(scores, rule_map):\n",
    "    max_rule = max(scores, key=scores.get)\n",
    "    label = rule_map[max_rule]\n",
    "    return max_rule, label, scores[max_rule]\n",
    "\n",
    "# if keyword found, get context window for similarity scoring\n",
    "def get_context(text, keywords):\n",
    "    n = 4\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9]+', ' ', text)\n",
    "\n",
    "    words = text.split()\n",
    "    found_index = [i for i, w in enumerate(words) if any(k.strip() in w for k in keywords)]\n",
    "    context = [\" \".join(words[max(0, idx-n):min(idx+n+1, len(words))]) for idx in found_index]\n",
    "\n",
    "    return '|'.join(context)\n",
    "\n",
    "## helper function to run CREAM on all data points\n",
    "def __helper__(row):\n",
    "    global keywords\n",
    "    global model\n",
    "    global rule_map\n",
    "    global encoded_rules\n",
    "    \n",
    "    THRESHOLD = 0.9\n",
    "        \n",
    "    text = row[\"text\"]\n",
    "    context = get_context(text, keywords).split('|')\n",
    "    \n",
    "    if len(context) > 0 and context[0] != \"\":\n",
    "        all_scores = []\n",
    "        for c in context:\n",
    "            scores = get_sim(model, rule_map, encoded_rules, c)\n",
    "            all_scores.append(label_from_max(scores, rule_map))\n",
    "        max_score = max(all_scores, key=itemgetter(2))\n",
    "        if max_score[2] >= THRESHOLD:\n",
    "            return max_score[1], max_score[2], max_score[0]\n",
    "        else:\n",
    "            return None, 0, None\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654a45f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load sample keywords, rules, and data\n",
    "keywords = [\n",
    "    \"bachelor\",\n",
    "    \"master\",\n",
    "    \"degree\",\n",
    "    \"high school\",\n",
    "    \"education\",\n",
    "    \"diploma\",\n",
    "    \"ged\",\n",
    "    \"certification\"\n",
    "]\n",
    "\n",
    "rules = pd.read_csv(\"data/demo/CREAM_ed_requirements_onet_coded.csv\")\n",
    "encoded_rules = model.encode(rules['rule'].tolist())\n",
    "rule_map = dict(zip(rules['rule'].tolist(), rules['category'].tolist()))\n",
    "\n",
    "data[['ed_inferred_rule', 'ed_inferred_label', 'ed_inferred_confidence']] = data.apply(__helper__, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4eb9e",
   "metadata": {},
   "source": [
    "# JobTag\n",
    "\n",
    "##CRAML and CREAM built binary ML classifiers that indicate niche job ad features (telework, independent contractor, etc.).\n",
    "\n",
    "Classes currently available:   \n",
    "\n",
    "List of Keys (Classes):\n",
    "* wfh\n",
    "* ind_contractor\n",
    "* proflicenses\n",
    "* driverslicense\n",
    "* yesunion\n",
    "* GovContract\n",
    "* CitizenshipReq\n",
    "* WorkAuthReq\n",
    "* VisaInclude\n",
    "* VisaExclude\n",
    "\n",
    "See [https://zenodo.org/records/7454652](Zenodo repository) and the following for details on the classifiers:\n",
    "\n",
    "```\n",
    "    Meisenbacher, Stephen, and Peter Norlander. 2023. “Transforming Unstructured Text into Data with Context Rule Assisted Machine Learning (CRAML).” arXiv. https://doi.org/10.48550/arXiv.2301.08549.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from JAAT import JobTag\n",
    "\n",
    "# Initialize the JobTag class with the desired class name\n",
    "class_name = \"proflicenses\"  # Replace with the actual class name you want to use\n",
    "job_tagger = JobTag(class_name)\n",
    "\n",
    "# Example text to classify\n",
    "text = \"Your example job description or text here.\"\n",
    "\n",
    "# Get the tag for the single text\n",
    "tag, confidence = job_tagger.get_tag(text)\n",
    "print(f\"Tag: {tag}, Confidence: {confidence}\")\n",
    "\n",
    "# Example list of texts to classify in batch\n",
    "texts = [\n",
    "    \"Remote opportunity.\",\n",
    "    \"Independent contractor.\",\n",
    "    \"We do not sponsor H-1B visas.\",\n",
    "    \"U.S. Citizens and Permanent Residents only.\"\n",
    "]\n",
    "\n",
    "# Get tags for the batch of texts\n",
    "tags = job_tagger.get_tag_batch(texts)\n",
    "print(\"Batch Tags:\", tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize JobTag for each class and apply to the data\n",
    "for class_name in classes:\n",
    "    print(f\"Processing {class_name}...\")\n",
    "    job_tagger = JobTag(class_name)\n",
    "    \n",
    "    # Use get_tag_batch to process all texts at once\n",
    "    tags = job_tagger.get_tag_batch(data['text'].tolist())\n",
    "    \n",
    "    # Create a new column for each class\n",
    "    data[f'JobTag_{class_name}'] = [tag[1] for tag in tags]  # tag[1] is the confidence score\n",
    "\n",
    "print(\"JobTag processing complete.\")\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Optional: Save the updated dataframe\n",
    "# data.to_csv('updated_data_with_jobtags.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d7cd7",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.to_excel('data/demo/coded_output.xlsx', index=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11603c5-5f4f-4e64-ba5e-8103db39ae3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
